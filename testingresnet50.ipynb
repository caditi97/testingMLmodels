{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "testingresnet50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTQ4z-oOu40N",
        "colab_type": "text"
      },
      "source": [
        "# Load model from .pb file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSOmAtiu40Q",
        "colab_type": "text"
      },
      "source": [
        "This code will load and return the graph.\n",
        "\n",
        "* `model_file_name`: path to a `.pb` file.\n",
        "* `node_mapping`: mapping from the name of node in the graph to a Tensorflow node in current session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xa-jO4Vu40R",
        "colab_type": "code",
        "outputId": "e1281e9e-cbdb-4c14-bdd4-bee7cc13b384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow.python.platform import gfile\n",
        "def load_graph(sess, model_file_name, node_mapping={}):\n",
        "    with gfile.GFile(model_file_name, 'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        tf.import_graph_def(graph_def, node_mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NafMpZESu40Z",
        "colab_type": "text"
      },
      "source": [
        "This code will load the top-tagging model weights.\n",
        "\n",
        "Please change the `model_file_name` to the file name of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn6RS4R-u40a",
        "colab_type": "code",
        "outputId": "72c5a42b-4a6a-455b-e797-748fdbba2f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior() \n",
        "with tf.Session() as sess:\n",
        "    # Create a placeholder for the input\n",
        "    input_node = tf.placeholder(tf.float32, shape = [None, 224, 224, 3], name='Placeholder')\n",
        "    graph_def = load_graph(sess=sess,\n",
        "                           model_file_name='constantgraph.pb',\n",
        "                           node_mapping={'Placeholder': input_node})\n",
        "# Get the node for output\n",
        "output_node = tf.get_default_graph().get_tensor_by_name(\"import/classifier/model_1/classifier_output/Softmax:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4243d7ce1e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     graph_def = load_graph(sess=sess,\n\u001b[1;32m      5\u001b[0m                            \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constantgraph.pb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                            node_mapping={'Placeholder': input_node})\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the node for output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import/classifier/model_1/classifier_output/Softmax:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-0819aa89af02>\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m(sess, model_file_name, node_mapping)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDecodeError\u001b[0m: Error parsing message"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKlJIEVeu40f",
        "colab_type": "text"
      },
      "source": [
        "# Export to Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IZ-m6abu40g",
        "colab_type": "text"
      },
      "source": [
        "This code will save the graph into a logging directory so that we can inspect it with Tensorboard.\n",
        "\n",
        "* `log_dir_name`: Path to the directory that is used to store the log. You can use `tensorboard --logdir <log_dir_name>` to inspect the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NVBC-ohu40j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_to_log(log_dir_name):\n",
        "    LOGDIR='log-top-tagging-resnet50-2'\n",
        "    train_writer = tf.summary.FileWriter(log_dir_name)\n",
        "    train_writer.add_graph(sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWarhjKHu40m",
        "colab_type": "text"
      },
      "source": [
        "This code will save the top-tagging model to a folder.\n",
        "\n",
        "Please change the parameter to where you want to store the Tensorboad information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjMCzZOBu40o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_to_log('log-top-tagging-resnet50')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZkknadOu40r",
        "colab_type": "text"
      },
      "source": [
        "# Run an inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmuNZlz-u40s",
        "colab_type": "text"
      },
      "source": [
        "This code will run an inference on a matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BALjH5RUu40t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "with tf.Session() as sess:\n",
        "    y = sess.run(output_node, feed_dict={input_node: np.zeros((2, 224, 224, 3))})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w2B8CxJu40v",
        "colab_type": "code",
        "outputId": "7ffc2f94-1682-4a4b-b777-695608990e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00170122 0.9982988 ]\n",
            " [0.00170122 0.9982988 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1M4Kbx0u40z",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj01otnmu400",
        "colab_type": "text"
      },
      "source": [
        "Those are helper functions from [util.py](https://github.com/nhanvtran/MachineLearningNotebooks/blob/nvt/bwcustomweights-validate/project-brainwave/utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kX4_UWQu401",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_and_rgb(images): \n",
        "    import numpy as np\n",
        "    #normalize image to 0-255 per image.\n",
        "    image_sum = 1/np.sum(np.sum(images,axis=1),axis=-1)\n",
        "    given_axis = 0\n",
        "    # Create an array which would be used to reshape 1D array, b to have \n",
        "    # singleton dimensions except for the given axis where we would put -1 \n",
        "    # signifying to use the entire length of elements along that axis  \n",
        "    dim_array = np.ones((1,images.ndim),int).ravel()\n",
        "    dim_array[given_axis] = -1\n",
        "    # Reshape b with dim_array and perform elementwise multiplication with \n",
        "    # broadcasting along the singleton dimensions for the final output\n",
        "    image_sum_reshaped = image_sum.reshape(dim_array)\n",
        "    images = images*image_sum_reshaped*255\n",
        "\n",
        "    # make it rgb by duplicating 3 channels.\n",
        "    images = np.stack([images, images, images],axis=-1)\n",
        "    \n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gukB2aDWu404",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_events(train_files):\n",
        "    import tables\n",
        "    n_events = 0\n",
        "    for train_file in train_files:\n",
        "        f = tables.open_file(train_file, 'r')\n",
        "        n_events += f.root.label.shape[0]\n",
        "        f.close()\n",
        "    return n_events"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3spQeNOu407",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunks(files, chunksize, max_q_size=4, shuffle=True): \n",
        "    \"\"\"Yield successive n-sized chunks from a and b.\"\"\" \n",
        "    import tables\n",
        "    import numpy as np\n",
        "    for train_file in files: \n",
        "        f = tables.open_file(train_file, 'r') \n",
        "        nrows = f.root.label.nrows\n",
        "        for istart in range(0,nrows,max_q_size*chunksize):  \n",
        "            a = np.array(f.root.img_pt[istart:istart+max_q_size*chunksize]) # Images \n",
        "            b = np.array(f.root.label[istart:istart+max_q_size*chunksize]) # Labels \n",
        "            if shuffle: \n",
        "                c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)] # shuffle within queue size\n",
        "                np.random.shuffle(c)\n",
        "                test_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
        "                test_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
        "            else:\n",
        "                test_images = a\n",
        "                test_labels = b\n",
        "            for jstart in range(0,len(test_labels),chunksize): \n",
        "                yield normalize_and_rgb(test_images[jstart:jstart+chunksize].copy()),test_labels[jstart:jstart+chunksize].copy(), len(test_labels[jstart:jstart+chunksize].copy())  \n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQHBqp_iu40-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(preds, in_images, test_files, chunk_size=64, shuffle=True):\n",
        "    \"\"\"Test the model\"\"\"\n",
        "    # import tensorflow as tf \n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior() \n",
        "    from keras import backend as K\n",
        "    from keras.objectives import binary_crossentropy \n",
        "    import numpy as np\n",
        "    from keras.metrics import categorical_accuracy\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "    \n",
        "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
        "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
        "    auc = tf.metrics.auc(tf.cast(in_labels, tf.bool), preds)\n",
        "   \n",
        "    n_test_events = count_events(test_files)\n",
        "    chunk_num = int(n_test_events/chunk_size)+1\n",
        "    preds_all = []\n",
        "    label_all = []\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    \n",
        "    avg_accuracy = 0\n",
        "    avg_auc = 0\n",
        "    avg_test_loss = 0\n",
        "    is_training = tf.get_default_graph().get_tensor_by_name('import/is_training:0')\n",
        "    n_current_events = 0\n",
        "    for img_chunk, label_chunk, real_chunk_size in chunks(test_files, chunk_size, shuffle=shuffle):\n",
        "        test_loss, accuracy_result, auc_result, preds_result = sess.run([cross_entropy, accuracy, auc, preds],\n",
        "                        feed_dict={in_images: img_chunk,\n",
        "                                   in_labels: label_chunk,\n",
        "                                   K.learning_phase(): 0,\n",
        "                                   is_training: False})\n",
        "        avg_test_loss += test_loss * real_chunk_size / n_test_events\n",
        "        avg_accuracy += accuracy_result * real_chunk_size / n_test_events\n",
        "        avg_auc += auc_result[0]  * real_chunk_size / n_test_events \n",
        "        preds_all.extend(preds_result)\n",
        "        label_all.extend(label_chunk)\n",
        "        n_current_events += real_chunk_size\n",
        "    \n",
        "        # print(\"test_loss = \", \"{:.3f}\".format(avg_test_loss*n_test_events/n_current_events), end=\"\")\n",
        "        # print(\"Test Accuracy:\", \"{:.3f}\".format(avg_accuracy*n_test_events/n_current_events), \", Area under ROC curve:\", \"{:.3f}\".format(avg_auc*n_test_events/n_current_events))\n",
        "    \n",
        "    return avg_test_loss, avg_accuracy, avg_auc, np.asarray(preds_all).reshape(n_test_events,2), np.asarray(label_all).reshape(n_test_events,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRwNo_CO2V3h",
        "colab_type": "code",
        "outputId": "fd3de1ba-1831-4fa8-de4f-72b932ebb992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "avg_test_loss, avg_acc, avg_auc, test_preds_t, test_labels_t = test_model(output_node, input_node, ['.test_file_0.h5'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0213cc4e3704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.test_file_0.h5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nG8KlXZu41C",
        "colab_type": "text"
      },
      "source": [
        "This will test the model on a test file.\n",
        "\n",
        "Please change the parameter so that it uses the correct file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQznJpM7xrpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Once results have been compiled, use this function to plot them.\n",
        "# It expects all the files to be there at runtime, so if they haven't yet been generated,\n",
        "# comment out the relevant lines.\n",
        "def plot_results(avg_test, avg_acc, avg_auc, test_preds_t, test_labels_t,plot_label='ROC.pdf'):\n",
        "    import os\n",
        "    import numpy as np\n",
        "    from sklearn import metrics\n",
        "\n",
        "    # Load the labels and results into memory.\n",
        "    # test_labels_t  = np.load(results_dir + \"/t_labels.npy\")\n",
        "    # test_preds_t   = np.load(results_dir + \"/t_preds.npy\")\n",
        "    # accuracy_q     = np.load(results_dir + \"/q_accuracy.npy\")\n",
        "    # test_labels_q  = np.load(results_dir + \"/q_labels.npy\")\n",
        "    # test_preds_q   = np.load(results_dir + \"/q_preds.npy\")\n",
        "    # test_labels_ft = np.load(results_dir + \"/ft_labels.npy\")\n",
        "    # test_preds_ft  = np.load(results_dir + \"/ft_preds.npy\")\n",
        "    # test_labels_b = np.load(results_dir + \"/b_labels.npy\")\n",
        "    # test_preds_b  = np.load(results_dir + \"/b_preds.npy\")\n",
        "    # test_labels_b_ft = np.load(results_dir + \"/b_labels.npy\")\n",
        "    # test_preds_b_ft  = np.load(results_dir + \"/b_newpreds.npy\")\n",
        "\n",
        "    new_test_preds_t = np.zeros(test_preds_t.shape)\n",
        "    new_test_preds_t[:,0] = test_preds_t[:,0]/np.sum(test_preds_t,axis=1)\n",
        "    new_test_preds_t[:,1] = test_preds_t[:,1]/np.sum(test_preds_t,axis=1)\n",
        "    test_preds_t = new_test_preds_t\n",
        "\n",
        "    # new_test_preds_q = np.zeros(test_preds_q.shape)\n",
        "    # new_test_preds_q[:,0] = test_preds_q[:,0]/np.sum(test_preds_q,axis=1)\n",
        "    # new_test_preds_q[:,1] = test_preds_q[:,1]/np.sum(test_preds_q,axis=1)\n",
        "    # test_preds_q = new_test_preds_q\n",
        "\n",
        "    # new_test_preds_ft = np.zeros(test_preds_ft.shape)\n",
        "    # new_test_preds_ft[:,0] = test_preds_ft[:,0]/np.sum(test_preds_ft,axis=1)\n",
        "    # new_test_preds_ft[:,1] = test_preds_ft[:,1]/np.sum(test_preds_ft,axis=1)\n",
        "    # test_preds_ft = new_test_preds_ft\n",
        "\n",
        "    # new_test_preds_b = np.zeros(test_preds_b.shape)\n",
        "    # new_test_preds_b[:,0] = test_preds_b[:,0]/np.sum(test_preds_b,axis=1)\n",
        "    # new_test_preds_b[:,1] = test_preds_b[:,1]/np.sum(test_preds_b,axis=1)\n",
        "    # test_preds_b = new_test_preds_b\n",
        "    \n",
        "    # new_test_preds_b_ft = np.zeros(test_preds_b_ft.shape)\n",
        "    # new_test_preds_b_ft[:,0] = test_preds_b_ft[:,0]/np.sum(test_preds_b_ft,axis=1)\n",
        "    # new_test_preds_b_ft[:,1] = test_preds_b_ft[:,1]/np.sum(test_preds_b_ft,axis=1)\n",
        "    # test_preds_b_ft = new_test_preds_b_ft\n",
        "    \n",
        "    accuracy_t = metrics.accuracy_score(test_labels_t[:,0], test_preds_t[:,0]>0.5)\n",
        "    # accuracy_q = metrics.accuracy_score(test_labels_q[:,0], test_preds_q[:,0]>0.5)\n",
        "    # accuracy_ft = metrics.accuracy_score(test_labels_ft[:,0], test_preds_ft[:,0]>0.5)\n",
        "    # accuracy_b = metrics.accuracy_score(test_labels_b[:,0], test_preds_b[:,0]>0.5)\n",
        "    # accuracy_b_ft = metrics.accuracy_score(test_labels_b_ft[:,0], test_preds_b_ft[:,0]>0.5)\n",
        "\n",
        "    # Determine the ROC curve for each of the tests. \n",
        "    # [:,0] will convert the labels from one-hot to binary.\n",
        "    fpr_test_t, tpr_test_t, thresholds      = metrics.roc_curve(test_labels_t[:,0],  test_preds_t[:,0])\n",
        "    # fpr_test_q, tpr_test_q, thresholds_q    = metrics.roc_curve(test_labels_q[:,0],  test_preds_q[:,0])\n",
        "    # fpr_test_ft, tpr_test_ft, thresholds_ft    = metrics.roc_curve(test_labels_ft[:,0],  test_preds_ft[:,0])\n",
        "    # fpr_test_b, tpr_test_b, thresholds_b    = metrics.roc_curve(test_labels_b[:,0],  test_preds_b[:,0])\n",
        "    # fpr_test_b_ft, tpr_test_b_ft, thresholds_b_ft    = metrics.roc_curve(test_labels_b_ft[:,0],  test_preds_b_ft[:,0])\n",
        "    \n",
        "    # Use the data we just generated to determine the area under the ROC curve.\n",
        "    # Use the data we just generated to determine the area under the ROC curve.\n",
        "    auc_test    = metrics.auc(fpr_test_t, tpr_test_t)\n",
        "    # auc_test_q  = metrics.auc(fpr_test_q, tpr_test_q)\n",
        "    # auc_test_ft  = metrics.auc(fpr_test_ft, tpr_test_ft)\n",
        "    # auc_test_b  = metrics.auc(fpr_test_b, tpr_test_b)\n",
        "    # auc_test_b_ft  = metrics.auc(fpr_test_b_ft, tpr_test_b_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ1d-mkAu41G",
        "colab_type": "text"
      },
      "source": [
        "# Load model from .h5 file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGT5w-gAu41H",
        "colab_type": "text"
      },
      "source": [
        "Change `file_name` to the path of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1v2Bn72u41I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "file_name = 'class_model_best.h5'\n",
        "new_model = keras.models.load_model(file_name)\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqgzm3Vku41L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}